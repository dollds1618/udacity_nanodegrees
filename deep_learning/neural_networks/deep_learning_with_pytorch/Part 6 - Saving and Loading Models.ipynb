{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "\n",
    "In this notebook, I'll show you how to save and load models with PyTorch. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "import fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 231,
       "width": 231
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a network\n",
    "\n",
    "To make things more concise here, I moved the model architecture and training code from the last part to a file called `fc_model`. Importing this, we can easily create a fully-connected network with `fc_model.Network`, and train the network using `fc_model.train`. I'll use this model (once it's trained) to demonstrate how we can save and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.677..  Test Loss: 0.999..  Test Accuracy: 0.647\n",
      "Epoch: 1/2..  Training Loss: 1.053..  Test Loss: 0.779..  Test Accuracy: 0.711\n",
      "Epoch: 1/2..  Training Loss: 0.891..  Test Loss: 0.682..  Test Accuracy: 0.745\n",
      "Epoch: 1/2..  Training Loss: 0.817..  Test Loss: 0.651..  Test Accuracy: 0.751\n",
      "Epoch: 1/2..  Training Loss: 0.755..  Test Loss: 0.632..  Test Accuracy: 0.757\n",
      "Epoch: 1/2..  Training Loss: 0.713..  Test Loss: 0.622..  Test Accuracy: 0.759\n",
      "Epoch: 1/2..  Training Loss: 0.679..  Test Loss: 0.584..  Test Accuracy: 0.778\n",
      "Epoch: 1/2..  Training Loss: 0.663..  Test Loss: 0.589..  Test Accuracy: 0.772\n",
      "Epoch: 1/2..  Training Loss: 0.670..  Test Loss: 0.557..  Test Accuracy: 0.791\n",
      "Epoch: 1/2..  Training Loss: 0.629..  Test Loss: 0.552..  Test Accuracy: 0.792\n",
      "Epoch: 1/2..  Training Loss: 0.665..  Test Loss: 0.551..  Test Accuracy: 0.796\n",
      "Epoch: 1/2..  Training Loss: 0.631..  Test Loss: 0.531..  Test Accuracy: 0.801\n",
      "Epoch: 1/2..  Training Loss: 0.596..  Test Loss: 0.520..  Test Accuracy: 0.805\n",
      "Epoch: 1/2..  Training Loss: 0.598..  Test Loss: 0.509..  Test Accuracy: 0.813\n",
      "Epoch: 1/2..  Training Loss: 0.637..  Test Loss: 0.529..  Test Accuracy: 0.806\n",
      "Epoch: 1/2..  Training Loss: 0.615..  Test Loss: 0.513..  Test Accuracy: 0.804\n",
      "Epoch: 1/2..  Training Loss: 0.606..  Test Loss: 0.499..  Test Accuracy: 0.813\n",
      "Epoch: 1/2..  Training Loss: 0.588..  Test Loss: 0.518..  Test Accuracy: 0.807\n",
      "Epoch: 1/2..  Training Loss: 0.606..  Test Loss: 0.504..  Test Accuracy: 0.814\n",
      "Epoch: 1/2..  Training Loss: 0.580..  Test Loss: 0.505..  Test Accuracy: 0.818\n",
      "Epoch: 1/2..  Training Loss: 0.601..  Test Loss: 0.498..  Test Accuracy: 0.822\n",
      "Epoch: 1/2..  Training Loss: 0.595..  Test Loss: 0.497..  Test Accuracy: 0.816\n",
      "Epoch: 1/2..  Training Loss: 0.556..  Test Loss: 0.478..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.548..  Test Loss: 0.480..  Test Accuracy: 0.820\n",
      "Epoch: 2/2..  Training Loss: 0.562..  Test Loss: 0.487..  Test Accuracy: 0.823\n",
      "Epoch: 2/2..  Training Loss: 0.559..  Test Loss: 0.488..  Test Accuracy: 0.818\n",
      "Epoch: 2/2..  Training Loss: 0.545..  Test Loss: 0.474..  Test Accuracy: 0.825\n",
      "Epoch: 2/2..  Training Loss: 0.577..  Test Loss: 0.494..  Test Accuracy: 0.825\n",
      "Epoch: 2/2..  Training Loss: 0.537..  Test Loss: 0.475..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.527..  Test Loss: 0.480..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.542..  Test Loss: 0.475..  Test Accuracy: 0.823\n",
      "Epoch: 2/2..  Training Loss: 0.532..  Test Loss: 0.455..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.514..  Test Loss: 0.449..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.526..  Test Loss: 0.456..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.509..  Test Loss: 0.467..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.522..  Test Loss: 0.454..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.522..  Test Loss: 0.464..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.558..  Test Loss: 0.491..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.589..  Test Loss: 0.468..  Test Accuracy: 0.828\n",
      "Epoch: 2/2..  Training Loss: 0.514..  Test Loss: 0.435..  Test Accuracy: 0.841\n",
      "Epoch: 2/2..  Training Loss: 0.500..  Test Loss: 0.450..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.515..  Test Loss: 0.451..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.502..  Test Loss: 0.453..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.549..  Test Loss: 0.449..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.503..  Test Loss: 0.445..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.507..  Test Loss: 0.437..  Test Accuracy: 0.840\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
    "\n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it to a file `'checkpoint.pth'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load the state dict with `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d859c59ebec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This will throw an error because the tensor sizes are wrong!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/udacity_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1052\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100])."
     ]
    }
   ],
   "source": [
    "# Try this\n",
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': 784,\n",
       " 'output_size': 10,\n",
       " 'hidden_layers': [400, 200, 100],\n",
       " 'state_dict': OrderedDict([('hidden_layers.0.weight',\n",
       "               tensor([[-0.0278,  0.0001,  0.0039,  ..., -0.0273,  0.0058,  0.0187],\n",
       "                       [-0.0230, -0.0293, -0.0342,  ...,  0.0144, -0.0109, -0.0207],\n",
       "                       [ 0.0113,  0.0178,  0.0288,  ..., -0.0304, -0.0298,  0.0115],\n",
       "                       ...,\n",
       "                       [-0.0116,  0.0040, -0.0099,  ...,  0.0337, -0.0311, -0.0222],\n",
       "                       [ 0.0050, -0.0104, -0.0042,  ..., -0.0339,  0.0293,  0.0178],\n",
       "                       [-0.0151, -0.0148,  0.0305,  ...,  0.0190, -0.0021, -0.0076]])),\n",
       "              ('hidden_layers.0.bias',\n",
       "               tensor([ 8.5609e-03, -1.2009e-02,  3.4218e-02, -1.2619e-02, -2.6445e-02,\n",
       "                        2.3079e-02, -2.1423e-02,  3.4011e-02, -6.1195e-03, -1.4708e-03,\n",
       "                       -2.9677e-02,  1.2155e-02,  1.3228e-02, -2.8925e-02,  9.7191e-03,\n",
       "                        1.1500e-02,  3.3621e-02, -2.4494e-02, -2.4255e-02,  2.6162e-03,\n",
       "                        2.1051e-02, -3.4299e-02, -1.1239e-02,  8.8572e-03, -3.2337e-02,\n",
       "                       -3.7461e-03,  1.9612e-02,  5.2496e-04,  2.4228e-02, -1.9630e-02,\n",
       "                        1.8978e-02, -2.8406e-02,  3.4414e-02, -3.1510e-02,  1.1340e-02,\n",
       "                        3.3996e-02,  2.3736e-02,  3.5687e-02, -1.1854e-02, -2.7359e-03,\n",
       "                        2.3354e-04,  6.9227e-03,  6.5338e-05,  1.4788e-02, -3.4466e-02,\n",
       "                       -1.7455e-02, -3.0903e-02,  3.4458e-02,  1.6256e-02,  9.9227e-03,\n",
       "                        2.9881e-02,  5.3866e-03, -9.6158e-03,  4.0970e-03, -1.7887e-02,\n",
       "                        5.1651e-03,  2.5474e-02, -2.8997e-02,  3.6559e-03, -1.8389e-02,\n",
       "                       -2.9961e-02,  9.4234e-03,  2.7884e-02, -1.1370e-02,  2.4395e-02,\n",
       "                        9.4433e-03,  3.4311e-02, -2.6121e-02, -2.1322e-02, -1.2098e-02,\n",
       "                       -3.1364e-02, -6.3660e-04,  7.5453e-03,  1.4579e-02, -2.6229e-03,\n",
       "                        3.0830e-02, -1.4327e-02, -2.4120e-02, -5.7712e-03,  4.3712e-03,\n",
       "                        3.1257e-02,  4.7938e-04,  1.1368e-02,  1.2126e-02, -2.3955e-02,\n",
       "                        1.4460e-02, -8.1017e-03,  3.5960e-03, -2.6364e-02,  2.4796e-02,\n",
       "                       -1.6771e-02,  8.3019e-03, -7.7756e-03,  3.3083e-02,  2.8008e-02,\n",
       "                        2.6234e-02,  2.3166e-03,  7.8462e-03,  8.3614e-03, -7.1972e-03,\n",
       "                        1.3979e-02, -1.1080e-02, -2.4856e-02,  1.7460e-02,  8.6313e-03,\n",
       "                       -3.2957e-02, -3.4155e-02,  8.2187e-03,  2.9257e-02, -6.3204e-04,\n",
       "                        2.9228e-02, -3.8809e-03,  3.6563e-03, -5.9354e-03, -2.1064e-02,\n",
       "                        1.5280e-02, -2.2321e-02, -1.7836e-02, -2.0164e-02,  2.4550e-02,\n",
       "                       -1.3393e-02, -1.4219e-02,  3.2452e-03, -3.2671e-02,  6.3372e-03,\n",
       "                        7.7313e-03,  1.0449e-03, -2.4183e-02, -1.0941e-03,  1.1543e-02,\n",
       "                       -2.6940e-02,  4.0449e-05,  4.8162e-03, -1.6882e-02, -1.9498e-02,\n",
       "                       -8.9079e-03, -3.5181e-02,  1.4884e-02,  1.7030e-02, -5.0163e-03,\n",
       "                        1.7933e-02,  4.2028e-03,  2.6710e-03, -1.4944e-02,  2.7859e-02,\n",
       "                        1.1538e-02,  3.3386e-02, -3.0995e-02,  3.5618e-02,  1.6327e-02,\n",
       "                        2.3791e-02, -2.6825e-03, -1.8365e-04, -2.2229e-02,  2.8725e-02,\n",
       "                        1.4797e-03,  3.0264e-02,  3.2020e-03, -1.6937e-03, -1.6564e-02,\n",
       "                       -3.2351e-02,  3.0731e-02,  1.7171e-02,  1.3424e-02, -8.3244e-03,\n",
       "                        2.4165e-03, -2.2761e-03,  2.1655e-02,  8.0305e-03,  3.0370e-03,\n",
       "                       -1.1694e-02, -1.5560e-02, -2.9299e-02, -8.9304e-03, -3.2770e-02,\n",
       "                        2.7090e-02,  3.1184e-03, -9.8914e-03,  6.6262e-03,  1.3788e-02,\n",
       "                       -2.5426e-02, -1.0008e-02, -1.9747e-02, -3.6660e-03, -1.6824e-02,\n",
       "                       -3.0641e-02,  8.2695e-03, -3.1795e-03,  2.8640e-02,  2.6009e-02,\n",
       "                       -2.3609e-02,  2.3612e-03, -8.7902e-03,  8.2250e-03,  3.4878e-02,\n",
       "                       -2.6091e-02,  1.3244e-02, -1.1355e-03,  2.7097e-02, -3.5467e-02,\n",
       "                        1.4587e-02,  3.3919e-02,  2.4385e-02, -9.1360e-03, -2.8998e-03,\n",
       "                        3.3345e-02, -2.0133e-02,  1.3572e-02,  1.1439e-03, -1.3673e-03,\n",
       "                        2.8146e-04, -2.1054e-02,  1.5267e-02,  2.4738e-02,  9.8780e-03,\n",
       "                       -1.2784e-02,  3.3260e-02,  9.7134e-03, -7.0392e-03, -7.0122e-03,\n",
       "                       -6.8169e-03, -2.1037e-03, -9.3642e-03,  1.0088e-02,  3.3262e-02,\n",
       "                        1.8825e-02, -1.5865e-02,  3.3089e-02,  1.8429e-02,  2.7394e-02,\n",
       "                       -3.5438e-02,  8.9943e-04, -1.4299e-04,  3.4206e-02,  1.8742e-02,\n",
       "                       -2.6666e-02,  3.0382e-02, -3.5301e-03,  1.6728e-02,  1.0599e-02,\n",
       "                       -2.4244e-02,  3.0763e-02, -1.9836e-02,  7.5204e-03,  2.4271e-02,\n",
       "                        3.1310e-02, -7.9271e-03, -1.5942e-02, -1.6817e-02, -9.7072e-03,\n",
       "                       -3.1467e-02, -1.3561e-02, -1.9258e-02, -9.6785e-03,  1.3857e-02,\n",
       "                       -4.1367e-03,  1.8649e-02,  1.0597e-02, -3.5599e-02, -8.7230e-03,\n",
       "                       -2.2513e-02, -6.7547e-04,  3.1404e-02,  1.9657e-02, -1.8836e-02,\n",
       "                        1.8473e-02,  9.0763e-03, -3.1839e-02,  3.1469e-02, -7.0826e-03,\n",
       "                        3.5101e-02, -7.0162e-03, -2.4735e-02, -2.7500e-02,  2.4845e-02,\n",
       "                       -9.4633e-03, -1.2791e-02, -5.6891e-03,  2.3644e-02, -1.5204e-02,\n",
       "                       -8.5985e-03, -2.0523e-02, -2.9939e-02,  3.5016e-02, -7.4571e-03,\n",
       "                        2.3167e-02,  3.4158e-02, -2.1081e-02, -3.3530e-02, -1.9364e-02,\n",
       "                        1.6593e-02, -2.6620e-02, -3.4671e-02, -1.6896e-02, -2.2035e-02,\n",
       "                       -2.2922e-02,  2.9177e-02, -1.2389e-02, -9.3067e-03, -2.3212e-02,\n",
       "                       -1.6467e-02, -1.9237e-02,  3.3136e-02,  1.0481e-02,  1.3958e-02,\n",
       "                       -5.8477e-03,  2.6210e-02,  2.0442e-02,  2.9779e-02, -1.6656e-02,\n",
       "                       -1.9918e-02, -2.7818e-03, -1.1836e-02,  8.2364e-03,  1.7409e-02,\n",
       "                       -3.5735e-03, -1.1353e-03, -2.9039e-02, -3.5255e-02,  1.2984e-03,\n",
       "                        3.0249e-02, -1.9025e-02,  1.9443e-02, -1.7473e-02,  6.6048e-04,\n",
       "                        4.0178e-03, -2.7741e-04, -9.4371e-03,  3.3491e-02, -7.2869e-03,\n",
       "                        1.9856e-02, -2.8658e-02,  1.4359e-02, -3.5761e-03,  1.2636e-02,\n",
       "                        3.1770e-02,  1.3010e-02,  1.6467e-02, -1.6228e-02, -2.6204e-02,\n",
       "                       -7.1529e-03,  2.2597e-02,  1.8279e-02, -1.3829e-02, -8.1908e-03,\n",
       "                        2.0374e-03, -2.4790e-02,  7.6269e-03,  2.7676e-02, -1.7838e-02,\n",
       "                        9.2998e-03, -5.5741e-03,  1.0169e-02, -1.1061e-02, -2.3184e-02,\n",
       "                        1.7157e-02,  3.1033e-02, -2.3008e-02, -6.4419e-03, -6.2010e-03,\n",
       "                        7.7889e-03,  7.1490e-03,  2.8852e-02, -2.2338e-02,  1.5419e-02,\n",
       "                        3.2919e-02,  1.9040e-02,  1.4960e-03, -2.6241e-02,  2.0919e-02,\n",
       "                        9.5668e-03,  2.3624e-02, -8.4497e-03, -1.1096e-02, -3.3220e-02,\n",
       "                       -1.9014e-02, -3.3806e-02, -6.1024e-03, -2.0556e-02,  2.1874e-02,\n",
       "                       -2.3131e-02,  4.3095e-03,  2.0212e-02, -9.2334e-03,  1.3627e-02,\n",
       "                        3.5589e-02, -1.2641e-02,  1.3749e-02, -9.6779e-03,  1.8385e-02,\n",
       "                       -5.5030e-03, -1.6732e-03,  2.6180e-02,  3.1127e-02, -2.8336e-02,\n",
       "                        8.6429e-03, -1.2428e-02, -3.9043e-03,  2.8575e-03, -2.1643e-02])),\n",
       "              ('hidden_layers.1.weight',\n",
       "               tensor([[-0.0306,  0.0295,  0.0083,  ...,  0.0398, -0.0129,  0.0245],\n",
       "                       [ 0.0163,  0.0399,  0.0392,  ..., -0.0464,  0.0335,  0.0296],\n",
       "                       [ 0.0385,  0.0237,  0.0219,  ..., -0.0053,  0.0409,  0.0084],\n",
       "                       ...,\n",
       "                       [ 0.0469, -0.0039, -0.0035,  ..., -0.0175, -0.0063, -0.0494],\n",
       "                       [-0.0475, -0.0465,  0.0447,  ...,  0.0288, -0.0282, -0.0079],\n",
       "                       [-0.0189, -0.0201,  0.0180,  ..., -0.0248, -0.0383, -0.0307]])),\n",
       "              ('hidden_layers.1.bias',\n",
       "               tensor([ 0.0348, -0.0484, -0.0234,  0.0150, -0.0372,  0.0103,  0.0005,  0.0145,\n",
       "                        0.0186,  0.0283, -0.0280, -0.0413,  0.0346,  0.0311, -0.0027,  0.0186,\n",
       "                        0.0205,  0.0091,  0.0472,  0.0268,  0.0115, -0.0149, -0.0378, -0.0318,\n",
       "                       -0.0385, -0.0329,  0.0040, -0.0459, -0.0427,  0.0458,  0.0245, -0.0252,\n",
       "                       -0.0059,  0.0420,  0.0344,  0.0094, -0.0086,  0.0140,  0.0384,  0.0233,\n",
       "                        0.0189,  0.0461, -0.0049,  0.0192,  0.0442, -0.0192, -0.0432, -0.0325,\n",
       "                        0.0348, -0.0263,  0.0169, -0.0271,  0.0105,  0.0409, -0.0175, -0.0333,\n",
       "                       -0.0397, -0.0208, -0.0254,  0.0349,  0.0140, -0.0017, -0.0110, -0.0462,\n",
       "                        0.0326, -0.0258,  0.0038, -0.0153, -0.0224, -0.0231, -0.0200, -0.0246,\n",
       "                        0.0257, -0.0147, -0.0275, -0.0020,  0.0240,  0.0245,  0.0495,  0.0040,\n",
       "                        0.0035, -0.0206, -0.0213, -0.0136,  0.0155, -0.0170,  0.0352, -0.0442,\n",
       "                        0.0059,  0.0158, -0.0464,  0.0437, -0.0215, -0.0429, -0.0391,  0.0081,\n",
       "                        0.0497, -0.0483,  0.0223,  0.0265, -0.0483, -0.0407, -0.0321,  0.0133,\n",
       "                        0.0013,  0.0291, -0.0311, -0.0479,  0.0238, -0.0264, -0.0375, -0.0222,\n",
       "                       -0.0430,  0.0139,  0.0138,  0.0020,  0.0388, -0.0148,  0.0057,  0.0133,\n",
       "                        0.0494,  0.0188, -0.0411,  0.0205, -0.0029, -0.0415, -0.0281,  0.0418,\n",
       "                        0.0159, -0.0253, -0.0448,  0.0387, -0.0229, -0.0202, -0.0162, -0.0324,\n",
       "                        0.0371, -0.0131, -0.0458,  0.0039,  0.0042,  0.0393, -0.0024, -0.0393,\n",
       "                        0.0191, -0.0353,  0.0057,  0.0189,  0.0231, -0.0233, -0.0407,  0.0440,\n",
       "                       -0.0499, -0.0118, -0.0063, -0.0037,  0.0484, -0.0465, -0.0056,  0.0160,\n",
       "                        0.0360,  0.0027,  0.0178,  0.0108,  0.0269, -0.0044,  0.0113,  0.0213,\n",
       "                       -0.0042, -0.0239, -0.0347, -0.0282,  0.0381, -0.0280, -0.0095, -0.0132,\n",
       "                        0.0464,  0.0269,  0.0365, -0.0321, -0.0162,  0.0324,  0.0424,  0.0247,\n",
       "                       -0.0450, -0.0415,  0.0440,  0.0222, -0.0005,  0.0255, -0.0227, -0.0466,\n",
       "                        0.0490,  0.0018,  0.0190, -0.0426, -0.0353,  0.0286, -0.0019, -0.0343])),\n",
       "              ('hidden_layers.2.weight',\n",
       "               tensor([[-0.0178,  0.0270,  0.0265,  ..., -0.0308,  0.0583, -0.0447],\n",
       "                       [ 0.0541, -0.0410, -0.0066,  ..., -0.0351, -0.0292, -0.0334],\n",
       "                       [-0.0194,  0.0563,  0.0272,  ...,  0.0676, -0.0496, -0.0677],\n",
       "                       ...,\n",
       "                       [ 0.0461, -0.0616, -0.0541,  ..., -0.0037, -0.0454, -0.0520],\n",
       "                       [ 0.0686,  0.0012, -0.0405,  ...,  0.0074,  0.0021,  0.0033],\n",
       "                       [ 0.0558,  0.0061, -0.0050,  ..., -0.0094,  0.0015,  0.0068]])),\n",
       "              ('hidden_layers.2.bias',\n",
       "               tensor([-1.1156e-02,  5.6995e-02,  6.1965e-02,  5.2776e-02, -5.6371e-02,\n",
       "                       -8.9361e-03,  6.4881e-03, -4.8057e-03, -3.3776e-03,  5.3304e-02,\n",
       "                        3.3101e-02,  4.2139e-02, -2.0391e-02,  4.0676e-02,  2.4885e-02,\n",
       "                       -5.0899e-02, -2.1463e-02, -4.1023e-05, -6.9082e-02,  3.4945e-02,\n",
       "                        1.6092e-02,  5.0774e-02, -3.5058e-02,  5.9601e-02, -9.5908e-04,\n",
       "                       -3.4305e-02,  1.9552e-02, -6.9074e-02, -4.8833e-02,  4.8788e-02,\n",
       "                       -1.4367e-02,  2.8068e-02, -5.1649e-02, -4.7247e-02, -6.8904e-02,\n",
       "                        6.1967e-02,  3.2075e-02,  4.6644e-02,  5.2397e-02,  5.1205e-02,\n",
       "                        2.3446e-02,  7.0212e-02, -5.5285e-02,  6.6536e-02,  6.1335e-02,\n",
       "                        3.5867e-03, -2.9333e-02,  5.5265e-02,  5.2749e-02, -2.2968e-02,\n",
       "                       -1.5071e-03, -6.4860e-02,  3.4811e-02, -2.9361e-02, -5.4561e-02,\n",
       "                        4.3910e-02, -5.6255e-02, -1.9107e-02,  1.0617e-02, -5.5571e-02,\n",
       "                        4.0982e-02, -2.9096e-02,  1.8409e-02, -4.6658e-02, -2.4809e-02,\n",
       "                        3.9043e-03, -7.0206e-02, -4.4352e-02,  4.1631e-02, -2.2359e-02,\n",
       "                        5.4386e-02, -1.0735e-02, -7.5938e-03, -4.3299e-02,  3.0371e-02,\n",
       "                        6.3429e-02,  6.6446e-02,  1.2100e-02, -2.8684e-02, -6.8400e-02,\n",
       "                       -5.7704e-02, -5.6072e-02,  1.4407e-02,  4.1624e-02,  6.6055e-02,\n",
       "                        3.0769e-02, -3.3694e-02,  4.4234e-03,  3.8335e-02, -1.9921e-02,\n",
       "                        3.8358e-02,  6.0142e-02, -1.4407e-02,  2.6043e-02,  2.3952e-03,\n",
       "                        1.1920e-02,  9.4400e-03, -1.9786e-02,  2.9068e-02,  4.1205e-02])),\n",
       "              ('output.weight',\n",
       "               tensor([[-1.2790e-02, -6.0308e-02,  3.3391e-02,  5.6112e-02,  8.4382e-02,\n",
       "                        -3.1773e-02,  3.9686e-02, -5.0473e-02,  5.6086e-02,  8.8227e-02,\n",
       "                         8.2974e-02,  5.8030e-02,  9.0995e-02, -1.9167e-02,  4.5874e-02,\n",
       "                         7.2024e-02,  3.0015e-02, -5.6809e-02, -8.0352e-03, -8.0927e-02,\n",
       "                         4.1779e-02,  1.7895e-02,  5.1056e-02,  4.4076e-02,  1.7842e-02,\n",
       "                        -7.7182e-02,  3.8061e-02,  8.0869e-02, -8.3256e-02,  6.9724e-02,\n",
       "                        -5.1139e-02,  9.7862e-02, -6.7256e-02, -3.6065e-02,  3.9527e-02,\n",
       "                        -5.7156e-02,  4.6182e-02, -4.0759e-04, -1.8477e-02,  6.0220e-02,\n",
       "                         6.7540e-02,  7.4212e-02, -4.6778e-02,  2.7497e-02, -7.2008e-02,\n",
       "                        -1.8331e-02,  2.5056e-02,  9.1363e-02, -2.4291e-02, -9.1494e-02,\n",
       "                         6.5250e-02, -6.6113e-02,  1.2101e-03,  4.2132e-02,  3.9275e-02,\n",
       "                        -4.6251e-02, -9.4339e-02,  1.1208e-02,  9.1233e-02, -6.4934e-03,\n",
       "                        -8.4592e-02, -7.4548e-02,  8.8443e-02,  4.3760e-02, -3.2078e-02,\n",
       "                        -4.7996e-02, -2.3491e-02, -9.6828e-02,  8.2340e-02,  3.6958e-02,\n",
       "                         1.5847e-02,  8.1341e-02,  2.2055e-02,  9.1361e-02, -1.4431e-02,\n",
       "                        -4.8551e-02, -9.2567e-02, -6.5801e-02,  2.2688e-02, -8.7166e-02,\n",
       "                         1.0993e-02, -5.1054e-02, -8.6962e-02, -1.1576e-02, -2.3310e-02,\n",
       "                        -5.5770e-02,  4.9339e-02, -1.5979e-02,  5.2645e-02,  6.8791e-02,\n",
       "                        -5.6417e-02, -1.7982e-02,  9.0013e-02,  6.1902e-02, -4.8890e-02,\n",
       "                        -3.5358e-02,  2.2165e-02, -7.1967e-02, -1.4256e-02, -7.4018e-02],\n",
       "                       [-5.9667e-02, -8.2789e-04,  7.3624e-02,  6.1003e-02, -4.5360e-02,\n",
       "                        -7.5957e-02, -2.8339e-03,  7.1325e-02, -2.1394e-02,  8.3011e-02,\n",
       "                         7.0726e-02, -9.0979e-02, -3.9118e-02, -9.1129e-03,  2.0870e-02,\n",
       "                         8.5631e-02, -3.9594e-02, -1.4301e-02, -3.3463e-02, -1.6593e-02,\n",
       "                         5.6150e-02, -4.4369e-02,  3.6184e-02,  8.6094e-02,  6.4593e-02,\n",
       "                        -8.5632e-02, -6.0389e-02,  3.0100e-02,  6.2295e-02, -1.6813e-02,\n",
       "                         7.2596e-02, -6.1695e-02, -3.0345e-02,  7.9902e-02,  1.2870e-02,\n",
       "                         5.6263e-02, -3.4317e-02, -8.6230e-02,  1.7931e-02,  8.7061e-03,\n",
       "                         4.2572e-02,  4.2024e-02, -2.1970e-02, -7.7086e-03,  7.5169e-02,\n",
       "                        -2.4881e-02, -4.5702e-02, -6.7376e-02,  7.0867e-02,  1.0458e-02,\n",
       "                        -9.2440e-02, -7.5063e-02,  8.7356e-02, -1.6633e-02, -4.2073e-03,\n",
       "                         4.7111e-02, -1.2669e-02, -4.1085e-02,  9.8847e-02, -4.0484e-02,\n",
       "                        -7.9935e-03, -1.3094e-02, -8.9285e-02,  6.7681e-02,  2.9631e-02,\n",
       "                         9.1247e-02,  2.3822e-02,  3.2817e-02, -5.1776e-02,  7.0100e-02,\n",
       "                        -1.5672e-02,  9.1521e-02, -6.8443e-02,  9.0406e-04, -4.7329e-02,\n",
       "                        -8.2348e-03,  1.3491e-02,  6.3164e-02, -4.3349e-02,  7.5076e-03,\n",
       "                        -8.9281e-02,  9.6027e-02,  6.7244e-02,  5.7604e-02, -3.3775e-02,\n",
       "                        -5.0213e-02, -6.4693e-03,  8.7214e-02, -8.5475e-02, -2.4381e-02,\n",
       "                         3.9464e-02, -2.7353e-02, -8.1886e-02, -1.6586e-02,  9.5291e-02,\n",
       "                        -2.1623e-02, -8.6512e-02, -5.3410e-02, -7.5662e-02, -8.8779e-02],\n",
       "                       [-9.1996e-02,  6.0135e-02,  2.4526e-03,  2.4934e-02, -5.2150e-03,\n",
       "                         5.4395e-02,  1.9515e-02,  4.9602e-02, -2.3494e-02, -8.3331e-02,\n",
       "                         5.7421e-02,  5.4045e-02,  4.6739e-02, -4.2189e-02,  4.1687e-02,\n",
       "                        -6.3932e-02,  1.7293e-02, -3.4268e-02,  2.0962e-02, -7.3028e-02,\n",
       "                         6.0603e-03,  4.2780e-02,  7.0236e-02, -4.6506e-02,  2.4532e-02,\n",
       "                         4.9637e-02, -7.8448e-02, -7.4749e-02,  3.0425e-02,  8.4110e-02,\n",
       "                        -4.5698e-02,  7.7768e-02,  2.5877e-02,  8.5669e-02,  2.8550e-02,\n",
       "                         3.8623e-02, -7.5859e-02, -3.2887e-02,  4.9292e-02,  2.4885e-02,\n",
       "                        -2.8722e-02,  2.8461e-02, -8.7969e-02, -8.2185e-02,  6.3537e-02,\n",
       "                         5.6530e-02,  3.0229e-02, -5.0489e-02, -6.4422e-02,  6.4876e-03,\n",
       "                         6.0345e-02,  7.6749e-02,  4.2302e-02, -2.7698e-02, -5.1672e-02,\n",
       "                         4.6039e-02,  1.8398e-02, -4.8149e-02,  1.9153e-02, -8.2172e-02,\n",
       "                         1.7341e-02,  2.1830e-02, -6.0023e-02, -4.4313e-02, -7.9192e-02,\n",
       "                        -3.9508e-02, -1.7308e-02, -1.3385e-02,  5.2969e-02, -8.3188e-02,\n",
       "                         2.5850e-02, -9.3212e-02,  5.9219e-02, -4.1340e-02, -1.4954e-02,\n",
       "                         8.4374e-02,  3.2194e-02, -5.2138e-02,  2.3182e-02, -9.9498e-02,\n",
       "                         4.2916e-02, -1.2444e-02,  8.8895e-02, -8.1704e-02,  6.9408e-02,\n",
       "                         7.5586e-02, -8.0273e-02, -9.9751e-02, -5.6789e-02, -9.9277e-02,\n",
       "                        -7.9003e-02,  8.0626e-02,  4.2149e-02, -3.6537e-02,  1.5729e-02,\n",
       "                         3.4043e-02,  8.8751e-02,  9.0847e-02,  2.5043e-02, -4.1474e-02],\n",
       "                       [ 6.7158e-02, -6.4115e-02,  5.0824e-02,  3.5359e-02, -1.3835e-03,\n",
       "                         1.5419e-02,  3.6955e-02,  7.7760e-02,  3.1510e-03,  7.9373e-02,\n",
       "                        -9.8886e-02,  6.8452e-02,  9.6888e-02, -2.5180e-02,  7.9433e-02,\n",
       "                         5.7580e-02, -2.1768e-02,  7.9024e-02,  6.3638e-02, -4.4705e-02,\n",
       "                         3.8997e-02, -4.0603e-02, -5.1186e-02,  3.0663e-02, -5.2939e-02,\n",
       "                         2.4017e-02,  8.1094e-02, -5.7299e-02, -5.8724e-02,  3.6557e-02,\n",
       "                        -7.1328e-02, -4.2054e-02, -4.7758e-02, -1.6547e-02, -9.2645e-02,\n",
       "                        -3.5091e-02, -6.3191e-02, -5.3235e-03, -3.1854e-02, -2.7962e-02,\n",
       "                         9.0013e-02,  5.4466e-02,  4.1327e-02, -2.3051e-02,  4.6023e-02,\n",
       "                        -4.8795e-02,  6.1799e-02, -5.2183e-02, -1.8236e-02,  3.3469e-02,\n",
       "                        -1.9634e-02, -8.1582e-02,  8.3193e-02, -1.1989e-03,  6.6766e-02,\n",
       "                        -1.8194e-02,  4.3531e-02, -9.9451e-03, -7.7311e-02,  3.8321e-02,\n",
       "                         5.1065e-02, -3.6676e-02, -6.4744e-02,  6.7956e-02, -1.0803e-02,\n",
       "                        -4.4010e-02, -2.6255e-02,  2.4346e-02,  8.1006e-02, -9.3631e-02,\n",
       "                        -8.7465e-02, -2.1192e-02, -1.3070e-02, -3.3620e-02, -6.3342e-02,\n",
       "                        -1.1590e-02,  5.9409e-02,  3.0945e-02, -2.1246e-02, -8.0762e-02,\n",
       "                        -1.1562e-03,  7.1491e-02, -7.6734e-02, -5.5584e-03,  6.4726e-02,\n",
       "                        -3.8010e-02, -3.0677e-02,  8.1766e-02,  5.6841e-02,  3.1659e-02,\n",
       "                         7.0859e-02, -3.2715e-04, -9.4450e-02,  9.2428e-02,  6.9654e-02,\n",
       "                        -4.7031e-02,  6.8732e-02,  5.5250e-02,  8.6747e-02, -6.4818e-02],\n",
       "                       [ 1.2237e-02,  5.2223e-02,  2.0281e-02, -2.9409e-02, -1.2306e-02,\n",
       "                         2.7605e-02,  2.6100e-02, -4.0002e-03,  2.4149e-02,  5.1532e-02,\n",
       "                         7.8971e-02,  6.0477e-02,  7.5701e-02, -3.9227e-02,  2.8120e-02,\n",
       "                        -9.9525e-02,  1.6321e-02,  5.2085e-02, -4.1498e-02,  4.6213e-02,\n",
       "                        -5.8730e-02, -7.4970e-03, -8.0191e-02,  4.9274e-02,  8.1922e-02,\n",
       "                         4.6726e-03,  5.0969e-02, -1.7157e-02,  3.5404e-02,  8.1004e-02,\n",
       "                        -1.0063e-02, -4.4030e-02,  5.2711e-02, -7.6269e-02, -1.9860e-02,\n",
       "                         6.2797e-02,  1.0361e-02, -5.3590e-02, -7.6836e-02, -6.7514e-02,\n",
       "                        -4.6380e-03,  2.1114e-02,  7.3764e-02,  8.9381e-02, -3.4969e-02,\n",
       "                        -9.3457e-02,  9.2161e-02, -5.7437e-03,  4.1870e-02,  6.5158e-02,\n",
       "                         9.9562e-02,  2.3400e-02, -2.4696e-02,  6.7508e-02, -7.1006e-02,\n",
       "                         9.3215e-02,  1.2108e-02, -3.6553e-02, -8.6031e-03,  8.6284e-02,\n",
       "                         3.6128e-02,  1.0870e-02,  6.4808e-02, -9.3733e-02, -6.9826e-02,\n",
       "                         4.9368e-02,  7.8400e-02, -2.7917e-02,  8.9144e-02,  2.1644e-02,\n",
       "                         2.1814e-02,  2.0852e-02, -8.3460e-02,  2.7410e-02, -8.6739e-02,\n",
       "                         2.6826e-02, -2.4079e-02, -9.9975e-02, -8.1507e-02, -2.2828e-02,\n",
       "                        -7.2064e-02,  1.1274e-02, -1.0223e-02,  9.7256e-02,  4.0804e-02,\n",
       "                        -1.5499e-02, -4.1306e-02, -4.8374e-02, -6.4493e-02,  6.9899e-02,\n",
       "                        -4.5634e-02,  8.5276e-02, -3.2310e-02, -7.8202e-02,  8.2242e-02,\n",
       "                         4.3863e-03,  1.0239e-02, -7.3542e-02, -6.8250e-02,  7.7913e-02],\n",
       "                       [-4.9692e-02,  4.9775e-02, -6.8548e-02, -9.7877e-02, -9.4992e-02,\n",
       "                        -3.7272e-03,  6.8382e-02,  8.7159e-02,  6.9517e-03,  8.8311e-03,\n",
       "                         3.4031e-02, -3.2879e-02, -3.7821e-02, -8.5244e-02,  6.5180e-02,\n",
       "                         7.0635e-02, -4.6312e-02,  4.4956e-03,  3.5937e-02, -4.8840e-03,\n",
       "                         9.0427e-02,  8.3869e-02,  7.6606e-02, -2.2783e-02,  4.7174e-02,\n",
       "                         9.4478e-02, -3.0964e-02,  2.6130e-02,  7.8934e-02, -1.0976e-03,\n",
       "                         6.2840e-02, -4.9681e-02, -7.7650e-02, -4.3505e-02, -1.8309e-02,\n",
       "                         6.6861e-02, -1.9187e-02,  7.9312e-02,  8.3561e-02,  2.7491e-02,\n",
       "                        -7.4782e-02,  7.3350e-02,  5.9920e-02, -4.3919e-02,  7.7389e-02,\n",
       "                         2.7288e-02, -8.5573e-02,  9.3126e-02, -3.1621e-02, -6.4436e-02,\n",
       "                         4.4507e-02,  7.3105e-02, -8.4325e-02,  2.9723e-02, -5.6258e-02,\n",
       "                        -1.0525e-02, -2.1647e-02,  6.0097e-02, -6.5330e-03,  8.1866e-02,\n",
       "                        -5.8367e-02, -5.5106e-02,  5.5898e-02, -1.6403e-02, -7.3067e-02,\n",
       "                         3.5435e-02, -5.1563e-02,  8.6417e-02,  9.5720e-02,  3.3395e-02,\n",
       "                         8.2260e-02,  8.7644e-02,  9.1360e-02,  2.8207e-02, -3.4559e-02,\n",
       "                        -9.6883e-02, -5.7619e-02,  3.4370e-02, -8.8188e-02, -9.5545e-02,\n",
       "                        -6.3573e-04,  1.0412e-02,  1.5513e-02,  2.1114e-02,  7.1081e-02,\n",
       "                         8.8004e-02, -9.6083e-03, -6.9761e-03,  4.0749e-02,  1.7065e-03,\n",
       "                        -4.1615e-02, -5.2903e-02, -9.3803e-06,  2.4701e-02, -6.5646e-03,\n",
       "                        -7.3119e-02, -8.3730e-02,  3.8956e-02,  2.3580e-02, -2.1496e-02],\n",
       "                       [ 1.8366e-02,  1.8338e-02, -5.5804e-03,  2.0585e-02,  8.6965e-02,\n",
       "                         1.5294e-02, -6.3148e-02,  2.4003e-02,  1.0501e-02,  9.0878e-04,\n",
       "                        -1.5235e-02,  9.3395e-02, -2.5098e-03, -7.7635e-03,  7.7383e-02,\n",
       "                        -6.1308e-02, -1.8474e-02,  9.4741e-02, -2.3351e-02, -7.1843e-02,\n",
       "                        -1.1750e-02, -6.2284e-02, -2.2640e-02,  7.8640e-02, -1.6711e-02,\n",
       "                         3.6527e-02, -5.5720e-02, -8.3242e-02,  2.5197e-02,  8.2515e-04,\n",
       "                        -2.5479e-02,  6.4697e-02, -5.1424e-02, -6.9332e-03, -4.2213e-02,\n",
       "                         2.7723e-02, -5.5028e-02,  7.1586e-02,  3.8803e-02, -2.8545e-02,\n",
       "                         1.4604e-02,  6.7413e-02,  2.5951e-02,  7.3483e-02, -4.0493e-02,\n",
       "                         6.6550e-02,  8.7164e-02, -4.0296e-02, -1.3452e-02, -4.4851e-02,\n",
       "                         7.3532e-02, -9.3373e-02,  3.5549e-02, -9.5959e-02,  5.0229e-02,\n",
       "                         6.5872e-02, -6.8000e-02,  4.0408e-02,  2.5480e-02, -3.8175e-02,\n",
       "                        -1.3146e-02, -2.9171e-02,  7.3046e-03,  4.0632e-02, -3.4581e-02,\n",
       "                        -5.4833e-02,  5.4303e-02, -2.7743e-02, -7.6798e-02, -7.3133e-02,\n",
       "                        -1.5955e-02,  5.4674e-02,  5.1737e-02, -7.5622e-02,  9.3416e-02,\n",
       "                         9.0911e-02,  1.9517e-02, -5.8980e-02, -4.2469e-02,  5.3324e-02,\n",
       "                         6.3172e-02, -4.0397e-02, -7.9685e-02,  8.8940e-02, -2.4146e-02,\n",
       "                         9.1468e-02, -7.5629e-02, -5.1634e-02, -6.5950e-02,  6.9121e-02,\n",
       "                         6.7678e-02, -7.0756e-02, -5.7863e-02,  1.1305e-03, -1.6383e-02,\n",
       "                         1.2640e-02, -8.3333e-02,  9.3777e-02, -5.5185e-03,  6.3274e-02],\n",
       "                       [ 5.2143e-02, -2.0751e-02,  7.3124e-02, -6.4050e-02,  7.7793e-02,\n",
       "                         6.5315e-02, -8.4545e-02,  4.9263e-02,  2.6991e-02,  4.5270e-02,\n",
       "                         6.7192e-02,  8.3211e-03, -9.2240e-02,  3.4961e-02, -6.7644e-02,\n",
       "                         1.3538e-02,  3.2305e-02, -4.4552e-02,  2.1835e-02,  4.3964e-02,\n",
       "                         8.4374e-04, -1.3203e-02,  2.7189e-02, -2.6691e-02, -6.4974e-02,\n",
       "                         7.6403e-02, -2.4745e-02, -1.2364e-03, -7.2164e-03, -1.3530e-02,\n",
       "                         6.9782e-02,  9.1336e-02,  1.7622e-02,  3.4489e-02, -2.1163e-04,\n",
       "                        -4.8103e-02,  9.7981e-02,  2.6969e-02,  6.6187e-02,  9.1351e-02,\n",
       "                         4.8956e-03,  2.0876e-02, -5.6151e-02,  6.3331e-02, -5.6723e-02,\n",
       "                         8.7466e-02,  4.6826e-02, -6.7486e-03,  5.1082e-02, -9.5946e-02,\n",
       "                         6.1403e-02, -1.5492e-02,  5.9845e-02, -9.7671e-02,  6.3815e-02,\n",
       "                         7.2426e-02, -7.7298e-02, -1.1158e-02, -8.4659e-02,  6.1837e-03,\n",
       "                         9.1733e-02,  9.8008e-02,  2.1951e-02, -4.0928e-02, -8.1389e-02,\n",
       "                         8.4533e-02, -7.1584e-02, -7.4684e-02,  2.0164e-02,  2.1373e-02,\n",
       "                        -6.1053e-03, -3.2553e-02, -3.3475e-02, -1.3795e-02, -4.2356e-02,\n",
       "                        -7.4657e-02, -4.1404e-02, -2.4743e-02,  7.7595e-02, -7.8733e-02,\n",
       "                        -6.9585e-02, -3.0789e-02,  5.8119e-02,  1.5847e-03, -1.2385e-02,\n",
       "                        -2.2423e-02, -7.7967e-02, -3.4699e-02,  2.6937e-02, -7.0453e-04,\n",
       "                         4.7352e-02, -2.6569e-02, -4.4701e-02, -5.6798e-03,  8.5296e-03,\n",
       "                        -8.7462e-02,  3.0829e-02, -5.9586e-02,  2.0660e-03, -5.9887e-02],\n",
       "                       [-4.3516e-02, -7.6789e-02, -5.6495e-02, -6.1748e-02, -5.9234e-02,\n",
       "                        -3.3307e-02,  7.3040e-02, -9.2982e-02, -3.7280e-02, -5.2220e-02,\n",
       "                         3.4582e-02,  8.7543e-02,  4.7959e-02, -8.6425e-03, -4.2488e-02,\n",
       "                         4.6642e-02,  3.9285e-03,  8.6415e-02,  1.9773e-02,  3.8659e-03,\n",
       "                         6.0043e-02,  1.5317e-02,  4.3775e-04, -2.6251e-02,  5.4582e-02,\n",
       "                        -9.5394e-02, -9.1030e-03,  7.5461e-02, -6.0217e-02, -5.1390e-03,\n",
       "                        -3.4827e-02, -3.0168e-02, -1.8616e-02,  2.4532e-02, -8.6044e-02,\n",
       "                        -7.8115e-02,  6.5899e-02,  6.6311e-02,  9.3900e-02,  9.5775e-02,\n",
       "                         1.2986e-02, -1.7668e-02,  5.3816e-02, -6.1377e-03,  2.6138e-02,\n",
       "                        -5.4299e-02,  1.2909e-03,  1.9608e-02,  8.8235e-02,  1.7560e-02,\n",
       "                         6.6558e-03, -6.5052e-02,  7.8620e-02,  4.1523e-02, -8.3054e-02,\n",
       "                        -7.8542e-02, -6.9321e-03, -9.2925e-02,  7.9194e-02, -8.9799e-02,\n",
       "                         9.8318e-02, -7.9743e-02, -2.6192e-02, -6.9886e-02,  8.4119e-02,\n",
       "                         6.0991e-02, -2.7360e-02,  9.6226e-02,  4.0125e-02,  8.0453e-02,\n",
       "                        -6.5761e-02,  4.0251e-02,  8.4711e-02,  1.4522e-02, -9.9899e-02,\n",
       "                        -3.5118e-02, -5.8597e-02,  3.6703e-02, -6.6835e-03,  1.7878e-02,\n",
       "                        -4.1608e-02, -8.6012e-02, -7.0677e-02,  2.0427e-02,  2.1623e-02,\n",
       "                        -9.3815e-03,  5.4765e-02,  7.6863e-02,  6.5756e-02,  4.9314e-02,\n",
       "                        -2.3826e-02,  7.8003e-02, -2.0797e-02, -1.6055e-02, -8.2176e-02,\n",
       "                         2.2716e-02, -5.9186e-02,  4.7612e-02,  4.7434e-03,  3.3831e-02],\n",
       "                       [-3.7873e-02,  4.6106e-03,  4.6542e-02, -1.0350e-03, -1.8280e-03,\n",
       "                         3.7633e-02,  7.2892e-02,  3.3096e-04, -3.9673e-02, -9.2479e-02,\n",
       "                        -1.3057e-02, -6.1915e-02,  4.9342e-02, -4.9416e-02,  2.5796e-02,\n",
       "                        -8.8469e-02, -6.9993e-02, -9.7665e-02,  9.0484e-03,  4.9807e-02,\n",
       "                        -3.3737e-02, -7.2714e-02, -5.9533e-03,  7.4964e-02, -1.4032e-02,\n",
       "                         5.5195e-02,  1.7219e-02, -1.8105e-02,  1.1901e-02,  9.5327e-02,\n",
       "                         7.4243e-02, -2.7307e-02,  9.6051e-02, -6.7374e-02, -3.9888e-02,\n",
       "                         7.5465e-02,  2.0396e-02, -1.3480e-02,  2.8289e-02, -4.6263e-03,\n",
       "                         9.1448e-02, -8.8711e-02,  9.2394e-03, -6.3855e-03,  1.4175e-02,\n",
       "                         8.9965e-02,  2.7298e-02,  8.4559e-02, -5.0417e-02, -9.0599e-03,\n",
       "                        -2.6570e-02,  7.5112e-02,  2.2414e-02, -5.4802e-02, -5.0807e-02,\n",
       "                         9.1686e-02, -6.4085e-02,  5.5554e-02,  6.6237e-02,  5.9300e-02,\n",
       "                        -6.9300e-02,  4.7343e-02, -7.4310e-02, -2.5651e-02, -1.3188e-02,\n",
       "                         2.5544e-02,  6.2698e-02, -7.8494e-02, -3.0438e-02,  9.8137e-02,\n",
       "                        -7.6918e-02, -8.0572e-02,  8.8036e-02,  6.0148e-02,  4.5088e-02,\n",
       "                         7.5705e-03,  2.0292e-02, -4.4674e-02,  3.1225e-02,  8.3333e-03,\n",
       "                         4.7350e-02, -5.7480e-02,  7.2010e-03,  4.9556e-02,  2.5353e-02,\n",
       "                         5.3909e-02,  4.9836e-02,  7.0851e-02, -5.6848e-02,  8.2383e-02,\n",
       "                        -9.9939e-02,  5.0056e-03, -2.4117e-02, -8.0011e-02,  8.9269e-02,\n",
       "                         7.8553e-03,  8.8347e-02, -7.5760e-02, -3.0883e-02, -6.5167e-02]])),\n",
       "              ('output.bias',\n",
       "               tensor([-0.0404, -0.1988,  0.0776,  0.1915,  0.0229, -0.0648,  0.0017, -0.1065,\n",
       "                       -0.0400, -0.1701]))])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ud_env",
   "language": "python",
   "name": "ud_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
